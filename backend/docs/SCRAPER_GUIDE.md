# åŸç¥è§’è‰²æ•°æ®çˆ¬è™«ç³»ç»Ÿä½¿ç”¨æŒ‡å—

## ğŸ“‹ ç›®å½•

- [ç³»ç»Ÿæ¶æ„](#ç³»ç»Ÿæ¶æ„)
- [çˆ¬å–è·¯å¾„å’Œé€»è¾‘](#çˆ¬å–è·¯å¾„å’Œé€»è¾‘)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [è¯¦ç»†ä½¿ç”¨æ­¥éª¤](#è¯¦ç»†ä½¿ç”¨æ­¥éª¤)
- [API æ¥å£è¯´æ˜](#api-æ¥å£è¯´æ˜)
- [æ•°æ®ç»“æ„](#æ•°æ®ç»“æ„)
- [é…ç½®è¯´æ˜](#é…ç½®è¯´æ˜)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## ç³»ç»Ÿæ¶æ„

### æ ¸å¿ƒç»„ä»¶

```
çˆ¬è™«ç³»ç»Ÿæ¶æ„
â”‚
â”œâ”€â”€ API å±‚ (src/api/scraper.py)
â”‚   â””â”€â”€ æä¾› REST API æ¥å£ï¼Œè§¦å‘å’Œç®¡ç†çˆ¬å–ä»»åŠ¡
â”‚
â”œâ”€â”€ çˆ¬è™«å±‚ (src/scrapers/)
â”‚   â”œâ”€â”€ character_scraper.py    # è§’è‰²æ•°æ®çˆ¬å–
â”‚   â”œâ”€â”€ base_scraper.py          # åŸºç¡€çˆ¬è™«åŠŸèƒ½
â”‚   â””â”€â”€ data_storage.py          # æ•°æ®å­˜å‚¨æœåŠ¡
â”‚
â”œâ”€â”€ æ•°æ®å±‚ (src/models/)
â”‚   â””â”€â”€ character.py             # è§’è‰²æ•°æ®æ¨¡å‹
â”‚
â””â”€â”€ æ•°æ®åº“ (PostgreSQL)
    â””â”€â”€ æŒä¹…åŒ–å­˜å‚¨è§’è‰²ä¿¡æ¯
```

---

## çˆ¬å–è·¯å¾„å’Œé€»è¾‘

### 1. URL æ¨¡å¼

**æ•°æ®æºï¼š** Bilibili åŸç¥ Wiki
```
https://wiki.biligame.com/ys/{è§’è‰²ä¸­æ–‡å}
```

**ç¤ºä¾‹ï¼š**
- ç´: `https://wiki.biligame.com/ys/%E7%90%B4`
- é›·ç”µå°†å†›: `https://wiki.biligame.com/ys/%E9%9B%B7%E7%94%B5%E5%B0%86%E5%86%9B`
- é‚£ç»´è±ç‰¹: `https://wiki.biligame.com/ys/%E9%82%A3%E7%BB%B4%E8%8E%B1%E7%89%B9`

### 2. çˆ¬å–æµç¨‹

```mermaid
graph TD
    A[ç”¨æˆ·è§¦å‘ API] --> B[åˆ›å»ºåå°ä»»åŠ¡]
    B --> C[åˆå§‹åŒ–çˆ¬è™«é…ç½®]
    C --> D[CharacterScraper.scrape]
    D --> E[éå†è§’è‰²åˆ—è¡¨]
    E --> F[ä¸ºæ¯ä¸ªè§’è‰²æ„å»º URL]
    F --> G[HTTP è¯·æ±‚è·å– HTML]
    G --> H[BeautifulSoup è§£æ HTML]
    H --> I[æå–åŸºç¡€ä¿¡æ¯]
    I --> J[æå–å±æ€§æ•°æ®]
    J --> K[æå–æè¿°]
    K --> L[æ•°æ®éªŒè¯]
    L --> M[å­˜å‚¨åˆ°æ•°æ®åº“]
    M --> N[è¿”å›ç»Ÿè®¡ç»“æœ]
```

### 3. æ•°æ®æå–é€»è¾‘

#### ç¬¬ä¸€æ­¥ï¼šHTTP è¯·æ±‚
```python
# ä½¿ç”¨ aiohttp å¼‚æ­¥è¯·æ±‚
url = f"https://wiki.biligame.com/ys/{quote(è§’è‰²å)}"
html = await fetch(url)
```

#### ç¬¬äºŒæ­¥ï¼šHTML è§£æ
```python
soup = BeautifulSoup(html, "lxml")
tables = soup.find_all("table", class_="wikitable")

# ç¬¬ä¸€ä¸ª wikitableï¼šåŸºç¡€ä¿¡æ¯
basic_info_table = tables[0]

# ç¬¬äºŒä¸ª wikitableï¼šå±æ€§æ•°æ®
stats_table = tables[1]
```

#### ç¬¬ä¸‰æ­¥ï¼šåŸºç¡€ä¿¡æ¯æå–
ä»ç¬¬ä¸€ä¸ª `wikitable` æå–ï¼š

| å­—æ®µ | æ¥æº | ç¤ºä¾‹ |
|------|------|------|
| **å…¨å** | `<th>å…¨å</th>` | ç´Â·å¤æ©å¸Œå°”å¾· (Jean Gunnhildr) |
| **ç¨€æœ‰åº¦** | `<th>ç¨€æœ‰åº¦</th>` çš„ `<img alt="5æ˜Ÿ.png">` | 5 |
| **å…ƒç´ ** | `<th>ç¥ä¹‹çœ¼</th>` æˆ– `<th>å¤é¾™å¤§æƒ</th>` | Anemo (é£) |
| **æ­¦å™¨ç±»å‹** | `<th>æ­¦å™¨ç±»å‹</th>` | Sword (å•æ‰‹å‰‘) |
| **åœ°åŒº** | `<th>æ‰€å±åœ°åŒº</th>` | Mondstadt (è’™å¾·) |

#### ç¬¬å››æ­¥ï¼šå±æ€§æ•°æ®æå–
ä»ç¬¬äºŒä¸ª `wikitable` æå– 90 çº§è§’è‰²å±æ€§ï¼š

**è¡¨æ ¼ç»“æ„ï¼š**
```
| ç­‰çº§ | ç”Ÿå‘½ä¸Šé™ |   | æ”»å‡»åŠ› |   | é˜²å¾¡åŠ› |   | æ²»ç–—åŠ æˆ |   |
|------|----------|---|--------|---|--------|---|----------|---|
|      | çªç ´å‰   |çªç ´å| çªç ´å‰  |çªç ´å| çªç ´å‰  |çªç ´å| çªç ´å‰    |çªç ´å|
| 90   | 14695    | - | 239    | - | 769    | - | 22.2%    | - |
```

**æå–é€»è¾‘ï¼š**
- è·³è¿‡ç¬¬ 1 è¡Œï¼ˆè¡¨å¤´ï¼‰
- è·³è¿‡ç¬¬ 2 è¡Œï¼ˆ"çªç ´å‰/çªç ´å" è¡Œï¼‰
- ä»ç¬¬ 3 è¡Œå¼€å§‹æ‰¾åˆ° `level == "90"` çš„è¡Œ
- æå– cells[1]=HP, cells[3]=ATK, cells[5]=DEF, cells[7]=çªç ´å±æ€§

#### ç¬¬äº”æ­¥ï¼šä¸­è‹±æ–‡æ˜ å°„
```python
# å…ƒç´ æ˜ å°„
"ç«" / "ç«å…ƒç´ " â†’ "Pyro"
"æ°´" / "æ°´å…ƒç´ " â†’ "Hydro"
"é£" / "é£å…ƒç´ " â†’ "Anemo"
"é›·" / "é›·å…ƒç´ " â†’ "Electro"
"è‰" / "è‰å…ƒç´ " â†’ "Dendro"
"å†°" / "å†°å…ƒç´ " â†’ "Cryo"
"å²©" / "å²©å…ƒç´ " â†’ "Geo"

# æ­¦å™¨æ˜ å°„
"å•æ‰‹å‰‘" / "å•æ‰‹å‰‘æ­¦å™¨ä½¿ç”¨" â†’ "Sword"
"åŒæ‰‹å‰‘" / "åŒæ‰‹å‰‘æ­¦å™¨ä½¿ç”¨" â†’ "Claymore"
"é•¿æŸ„æ­¦å™¨" / "é•¿æŸ„æ­¦å™¨æ­¦å™¨ä½¿ç”¨" â†’ "Polearm"
"å¼“" / "å¼“æ­¦å™¨ä½¿ç”¨" â†’ "Bow"
"æ³•å™¨" / "æ³•å™¨æ­¦å™¨ä½¿ç”¨" â†’ "Catalyst"

# åœ°åŒºæ˜ å°„
"è’™å¾·" â†’ "Mondstadt"
"ç’ƒæœˆ" â†’ "Liyue"
"ç¨»å¦»" â†’ "Inazuma"
"é¡»å¼¥" â†’ "Sumeru"
"æ«ä¸¹" â†’ "Fontaine"
"çº³å¡”" â†’ "Natlan"
"è‡³å†¬" â†’ "Snezhnaya"
```

---

## å¿«é€Ÿå¼€å§‹

### å‰ç½®æ¡ä»¶

1. **Python ç¯å¢ƒ**
```bash
Python 3.10+
```

2. **ä¾èµ–å®‰è£…**
```bash
cd backend
pip install -r requirements.txt
```

3. **æ•°æ®åº“å‡†å¤‡**
```bash
# å¯åŠ¨ PostgreSQL
docker-compose up -d postgres

# è¿è¡Œæ•°æ®åº“è¿ç§»
alembic upgrade head
```

4. **Redis å¯åŠ¨ï¼ˆå¯é€‰ï¼Œç”¨äºç¼“å­˜ï¼‰**
```bash
docker-compose up -d redis
```

### å¯åŠ¨åº”ç”¨

```bash
cd backend
uvicorn src.main:app --reload --port 8002
```

### è§¦å‘çˆ¬å–

#### æ–¹æ³• 1ï¼šä½¿ç”¨ curl
```bash
curl -X POST http://localhost:8002/api/scraper/characters/trigger
```

#### æ–¹æ³• 2ï¼šä½¿ç”¨æµè§ˆå™¨
è®¿é—® API æ–‡æ¡£é¡µé¢ï¼š
```
http://localhost:8002/api/docs
```

æ‰¾åˆ° `POST /api/scraper/characters/trigger`ï¼Œç‚¹å‡» "Try it out" â†’ "Execute"

#### æ–¹æ³• 3ï¼šä½¿ç”¨ Python
```python
import requests

response = requests.post("http://localhost:8002/api/scraper/characters/trigger")
print(response.json())
```

---

## è¯¦ç»†ä½¿ç”¨æ­¥éª¤

### Step 1: å¯åŠ¨åç«¯æœåŠ¡

```bash
# 1. è¿›å…¥åç«¯ç›®å½•
cd /Users/anker/Desktop/learn\ project/Speckit/genshin_wiki_information/backend

# 2. æ¿€æ´»è™šæ‹Ÿç¯å¢ƒï¼ˆå¦‚æœä½¿ç”¨ï¼‰
source venv/bin/activate  # macOS/Linux
# æˆ–
.\venv\Scripts\activate   # Windows

# 3. å¯åŠ¨ FastAPI åº”ç”¨
uvicorn src.main:app --reload --port 8002

# é¢„æœŸè¾“å‡ºï¼š
# INFO:     Uvicorn running on http://127.0.0.1:8002 (Press CTRL+C to quit)
# INFO:     Started reloader process [xxxxx] using StatReload
# INFO:     Started server process [xxxxx]
# INFO:     Waiting for application startup.
# INFO:     Application startup complete.
```

### Step 2: éªŒè¯ API å¯ç”¨

```bash
# æ£€æŸ¥ API å¥åº·çŠ¶æ€
curl http://localhost:8002/api/health

# é¢„æœŸè¿”å›ï¼š
# {"status": "healthy"}
```

### Step 3: æŸ¥çœ‹çˆ¬è™«é…ç½®

```bash
curl http://localhost:8002/api/scraper/config

# è¿”å›ç¤ºä¾‹ï¼š
{
  "success": true,
  "data": {
    "requests_per_second": 1.0,
    "max_retries": 3,
    "timeout_seconds": 30,
    "respect_robots_txt": true
  }
}
```

### Step 4: è§¦å‘è§’è‰²æ•°æ®çˆ¬å–

```bash
curl -X POST http://localhost:8002/api/scraper/characters/trigger

# è¿”å›ç¤ºä¾‹ï¼š
{
  "success": true,
  "message": "Character scraping task started in background",
  "status": "started"
}
```

### Step 5: æŸ¥çœ‹çˆ¬å–çŠ¶æ€

```bash
# å®æ—¶æŸ¥çœ‹çŠ¶æ€
curl http://localhost:8002/api/scraper/status

# çˆ¬å–è¿›è¡Œä¸­ï¼š
{
  "success": true,
  "data": {
    "is_running": true,
    "current_task": "characters",
    "last_run": "2025-11-10T08:30:00.000000",
    "last_result": null
  }
}

# çˆ¬å–å®Œæˆï¼š
{
  "success": true,
  "data": {
    "is_running": false,
    "current_task": null,
    "last_run": "2025-11-10T08:30:00.000000",
    "last_result": {
      "success": true,
      "scraper_stats": {
        "requests": 18,
        "errors": 0,
        "success_rate": 100.0
      },
      "storage_stats": {
        "created": 13,
        "updated": 5,
        "skipped": 0,
        "errors": 0
      },
      "total_characters": 18
    }
  }
}
```

### Step 6: æŸ¥çœ‹çˆ¬å–ç»“æœ

```bash
# æŸ¥è¯¢è§’è‰²åˆ—è¡¨
curl "http://localhost:8002/api/characters?page=1&page_size=10"

# è¿”å›ç¤ºä¾‹ï¼š
{
  "success": true,
  "data": {
    "items": [
      {
        "id": 1,
        "name": "ç´",
        "name_en": "Jean",
        "rarity": 5,
        "element": "Anemo",
        "weapon_type": "Sword",
        "region": "Mondstadt",
        "base_stats": {
          "hp": 14695,
          "atk": 239,
          "def": 769
        },
        "ascension_stats": {
          "stat": "healing_bonus",
          "value": 22.2
        }
      }
      // ... æ›´å¤šè§’è‰²
    ],
    "total": 18,
    "page": 1,
    "page_size": 10,
    "total_pages": 2
  }
}
```

### Step 7: æŸ¥çœ‹æ—¥å¿—

```bash
# æŸ¥çœ‹åº”ç”¨æ—¥å¿—ï¼ˆç»ˆç«¯è¾“å‡ºï¼‰
# æˆ–è€…æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶
tail -f logs/app.log  # å¦‚æœé…ç½®äº†æ—¥å¿—æ–‡ä»¶
```

æ—¥å¿—ç¤ºä¾‹ï¼š
```
INFO:     Starting character scraping task...
INFO:     Scraping character data...
INFO:     âœ… Scraped: ç´
INFO:     âœ… Scraped: è¿ªå¢å…‹
INFO:     âœ… Scraped: è«å¨œ
...
INFO:     Successfully scraped 18/18 characters
INFO:     Storing character data...
INFO:     Character storage complete. Created: 13, Updated: 5, Skipped: 0, Errors: 0
INFO:     Character scraping completed successfully.
```

---

## API æ¥å£è¯´æ˜

### 1. è§¦å‘è§’è‰²æ•°æ®çˆ¬å–

**æ¥å£ï¼š** `POST /api/scraper/characters/trigger`

**åŠŸèƒ½ï¼š** å¯åŠ¨åå°ä»»åŠ¡ï¼Œçˆ¬å–æ‰€æœ‰è§’è‰²æ•°æ®

**è¯·æ±‚ï¼š**
```bash
curl -X POST http://localhost:8002/api/scraper/characters/trigger
```

**å“åº”ï¼š**
```json
{
  "success": true,
  "message": "Character scraping task started in background",
  "status": "started"
}
```

**é”™è¯¯æƒ…å†µï¼š**
```json
{
  "detail": "Scraper is already running. Please wait for it to complete."
}
```

### 2. æŸ¥çœ‹çˆ¬è™«çŠ¶æ€

**æ¥å£ï¼š** `GET /api/scraper/status`

**åŠŸèƒ½ï¼š** è·å–çˆ¬è™«å½“å‰çŠ¶æ€

**è¯·æ±‚ï¼š**
```bash
curl http://localhost:8002/api/scraper/status
```

**å“åº”ï¼š**
```json
{
  "success": true,
  "data": {
    "is_running": false,
    "current_task": null,
    "last_run": "2025-11-10T08:30:00.000000",
    "last_result": {
      "success": true,
      "scraper_stats": {
        "requests": 18,
        "errors": 0,
        "success_rate": 100.0
      },
      "storage_stats": {
        "created": 13,
        "updated": 5,
        "skipped": 0,
        "errors": 0
      },
      "total_characters": 18
    }
  }
}
```

### 3. æŸ¥çœ‹çˆ¬è™«ç»Ÿè®¡

**æ¥å£ï¼š** `GET /api/scraper/stats`

**åŠŸèƒ½ï¼š** è·å–çˆ¬è™«ç»Ÿè®¡ä¿¡æ¯

**è¯·æ±‚ï¼š**
```bash
curl http://localhost:8002/api/scraper/stats
```

**å“åº”ï¼š**
```json
{
  "success": true,
  "data": {
    "last_run": "2025-11-10T08:30:00.000000",
    "is_running": false,
    "last_result": {
      "success": true,
      "total_characters": 18,
      "scraper_stats": {...},
      "storage_stats": {...}
    }
  }
}
```

### 4. æŸ¥çœ‹çˆ¬è™«é…ç½®

**æ¥å£ï¼š** `GET /api/scraper/config`

**åŠŸèƒ½ï¼š** è·å–å½“å‰çˆ¬è™«é…ç½®

**è¯·æ±‚ï¼š**
```bash
curl http://localhost:8002/api/scraper/config
```

**å“åº”ï¼š**
```json
{
  "success": true,
  "data": {
    "requests_per_second": 1.0,
    "max_retries": 3,
    "timeout_seconds": 30,
    "respect_robots_txt": true
  }
}
```

---

## æ•°æ®ç»“æ„

### Character æ•°æ®æ¨¡å‹

```python
{
  "id": 1,                          # è‡ªå¢ä¸»é”®
  "name": "ç´",                      # è§’è‰²ä¸­æ–‡å
  "name_en": "Jean",                # è§’è‰²è‹±æ–‡å
  "rarity": 5,                      # ç¨€æœ‰åº¦ (4 æˆ– 5)
  "element": "Anemo",               # å…ƒç´ ç±»å‹
  "weapon_type": "Sword",           # æ­¦å™¨ç±»å‹
  "region": "Mondstadt",            # æ‰€å±åœ°åŒº
  "description": "è¥¿é£éª‘å£«å›¢ä»£ç†å›¢é•¿...",  # è§’è‰²æè¿°

  # åŸºç¡€å±æ€§ (90çº§)
  "base_stats": {
    "hp": 14695,                    # ç”Ÿå‘½å€¼
    "atk": 239,                     # æ”»å‡»åŠ›
    "def": 769                      # é˜²å¾¡åŠ›
  },

  # çªç ´å±æ€§
  "ascension_stats": {
    "stat": "healing_bonus",        # å±æ€§ç±»å‹
    "value": 22.2                   # å±æ€§å€¼ (%)
  },

  "created_at": "2025-11-10T08:30:00",
  "updated_at": "2025-11-10T08:30:00"
}
```

### çªç ´å±æ€§ç±»å‹ (ascension_stats.stat)

| å€¼ | è¯´æ˜ | ç¤ºä¾‹è§’è‰² |
|----|------|---------|
| `crit_rate` | æš´å‡»ç‡ | è¿ªå¢å…‹ã€é­ˆ |
| `crit_dmg` | æš´å‡»ä¼¤å®³ | åˆ»æ™´ã€ç”˜é›¨ã€èƒ¡æ¡ƒ |
| `energy_recharge` | å…ƒç´ å……èƒ½æ•ˆç‡ | é›·ç”µå°†å†›ã€æ¸©è¿ªã€è«å¨œ |
| `healing_bonus` | æ²»ç–—åŠ æˆ | ç´ |
| `elemental_mastery` | å…ƒç´ ç²¾é€š | çº³è¥¿å¦²ã€é¦™è± |
| `physical_dmg_bonus` | ç‰©ç†ä¼¤å®³åŠ æˆ | ä¼˜èˆ |
| `elemental_dmg_bonus` | å…ƒç´ ä¼¤å®³åŠ æˆ | å®µå®« |
| `atk_percent` | æ”»å‡»åŠ›% | è¡Œç§‹ã€è²è°¢å°” |
| `anemo_dmg_bonus` | é£å…ƒç´ ä¼¤å®³åŠ æˆ | ç ‚ç³– |

---

## é…ç½®è¯´æ˜

### çˆ¬è™«é…ç½® (ScraperConfig)

ä½ç½®ï¼š`src/scrapers/base_scraper.py`

```python
@dataclass
class ScraperConfig:
    # é€Ÿç‡é™åˆ¶
    requests_per_second: float = 1.0        # æ¯ç§’è¯·æ±‚æ•°
    min_delay_seconds: float = 1.0          # æœ€å°å»¶è¿Ÿï¼ˆç§’ï¼‰
    max_delay_seconds: float = 3.0          # æœ€å¤§å»¶è¿Ÿï¼ˆç§’ï¼‰

    # é‡è¯•é…ç½®
    max_retries: int = 3                    # æœ€å¤§é‡è¯•æ¬¡æ•°
    retry_delay_seconds: float = 2.0        # é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
    retry_backoff_factor: float = 2.0       # æŒ‡æ•°é€€é¿å› å­

    # è¯·æ±‚è¶…æ—¶
    timeout_seconds: int = 30               # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

    # è¿æ¥æ± 
    max_connections: int = 10               # æœ€å¤§è¿æ¥æ•°
    max_connections_per_host: int = 5       # æ¯ä¸ªä¸»æœºæœ€å¤§è¿æ¥æ•°

    # User-Agent è½®æ¢
    user_agents: List[str] = [...]          # User-Agent åˆ—è¡¨

    # ä»£ç†é…ç½®ï¼ˆå¯é€‰ï¼‰
    proxy_url: Optional[str] = None         # ä»£ç† URL

    # éµå®ˆ robots.txt
    respect_robots_txt: bool = True         # æ˜¯å¦éµå®ˆ robots.txt
```

### ä¿®æ”¹é…ç½®

åœ¨ `src/api/scraper.py` çš„ `run_character_scraping` å‡½æ•°ä¸­ä¿®æ”¹ï¼š

```python
config = ScraperConfig(
    requests_per_second=2.0,    # æé«˜åˆ°æ¯ç§’ 2 ä¸ªè¯·æ±‚
    max_retries=5,              # å¢åŠ é‡è¯•æ¬¡æ•°
    timeout_seconds=60,         # å¢åŠ è¶…æ—¶æ—¶é—´
)
scraper = CharacterScraper(config)
```

### é»˜è®¤çˆ¬å–è§’è‰²åˆ—è¡¨

ä½ç½®ï¼š`src/scrapers/character_scraper.py` â†’ `scrape()` æ–¹æ³•

```python
character_names = [
    "ç´", "è¿ªå¢å…‹", "è«å¨œ", "æ¸©è¿ª",              # è’™å¾·
    "åˆ»æ™´", "é­ˆ", "ç”˜é›¨", "èƒ¡æ¡ƒ", "é’Ÿç¦»",        # ç’ƒæœˆ
    "é›·ç”µå°†å†›", "ç¥é‡Œç»«å",                     # ç¨»å¦»
    "çº³è¥¿å¦²",                                   # é¡»å¼¥
    "é‚£ç»´è±ç‰¹",                                 # æ«ä¸¹
    "ç­å°¼ç‰¹", "é¦™è±", "è¡Œç§‹", "ç ‚ç³–", "è²è°¢å°”",  # 4æ˜Ÿ
]
```

**å¦‚ä½•æ·»åŠ æ›´å¤šè§’è‰²ï¼š**
1. æ‰“å¼€ `character_scraper.py`
2. åœ¨ `scrape()` æ–¹æ³•çš„é»˜è®¤åˆ—è¡¨ä¸­æ·»åŠ è§’è‰²ä¸­æ–‡å
3. é‡å¯åº”ç”¨

---

## å¸¸è§é—®é¢˜

### Q1: çˆ¬å–å¤±è´¥æ€ä¹ˆåŠï¼Ÿ

**æ£€æŸ¥æ­¥éª¤ï¼š**

1. **æŸ¥çœ‹æ—¥å¿—**
```bash
# æŸ¥çœ‹åº”ç”¨è¾“å‡º
# æˆ–
tail -f logs/scraper.log
```

2. **æ£€æŸ¥ç½‘ç»œè¿æ¥**
```bash
curl -I https://wiki.biligame.com/ys/%E7%90%B4
```

3. **æŸ¥çœ‹é”™è¯¯ç»Ÿè®¡**
```bash
curl http://localhost:8002/api/scraper/status | jq '.data.last_result'
```

4. **æ‰‹åŠ¨æµ‹è¯•å•ä¸ªè§’è‰²**
```python
import asyncio
from src.scrapers.character_scraper import CharacterScraper

async def test():
    scraper = CharacterScraper()
    async with scraper:
        data = await scraper.scrape_character("ç´")
        print(data)

asyncio.run(test())
```

### Q2: å¦‚ä½•çˆ¬å–æŒ‡å®šè§’è‰²ï¼Ÿ

**æ–¹æ³• 1ï¼šä¿®æ”¹ä»£ç **
ç¼–è¾‘ `character_scraper.py`ï¼Œä¿®æ”¹é»˜è®¤è§’è‰²åˆ—è¡¨

**æ–¹æ³• 2ï¼šç›´æ¥è°ƒç”¨ï¼ˆéœ€è¦ç¼–å†™è„šæœ¬ï¼‰**
```python
import asyncio
from src.scrapers.character_scraper import CharacterScraper
from src.db.session import AsyncSessionLocal
from src.scrapers.data_storage import DataStorageService

async def scrape_custom_characters():
    scraper = CharacterScraper()

    # æŒ‡å®šè¦çˆ¬å–çš„è§’è‰²
    characters_to_scrape = ["ä¼˜èˆ", "å®µå®«", "æ«åŸä¸‡å¶"]

    async with scraper:
        characters = await scraper.scrape(characters_to_scrape)

    # å­˜å‚¨åˆ°æ•°æ®åº“
    async with AsyncSessionLocal() as db:
        storage = DataStorageService(db)
        stats = await storage.store_characters(characters)
        print(f"å­˜å‚¨å®Œæˆ: {stats}")

asyncio.run(scrape_custom_characters())
```

### Q3: æ•°æ®æ›´æ–°ç­–ç•¥æ˜¯ä»€ä¹ˆï¼Ÿ

çˆ¬è™«ä½¿ç”¨ **å¢é‡æ›´æ–°** ç­–ç•¥ï¼š

- **æ–°è§’è‰²**ï¼šç›´æ¥åˆ›å»º (`created`)
- **å·²å­˜åœ¨è§’è‰²**ï¼š
  - å¦‚æœæ•°æ®æœ‰å˜åŒ– â†’ æ›´æ–° (`updated`)
  - å¦‚æœæ•°æ®æ— å˜åŒ– â†’ è·³è¿‡ (`skipped`)

é€»è¾‘ä½äº `src/scrapers/data_storage.py`ï¼š

```python
# æ£€æŸ¥è§’è‰²æ˜¯å¦å­˜åœ¨
existing = await db.execute(
    select(Character).where(Character.name == char_data["name"])
)

if existing:
    # å¯¹æ¯”æ•°æ®ï¼Œå†³å®šæ˜¯å¦æ›´æ–°
    if has_changes(existing, char_data):
        update_character(existing, char_data)  # updated
    else:
        skip_character()  # skipped
else:
    create_character(char_data)  # created
```

### Q4: çˆ¬å–é€Ÿåº¦æ…¢æ€ä¹ˆåŠï¼Ÿ

**ä¼˜åŒ–æ–¹æ³•ï¼š**

1. **æé«˜è¯·æ±‚é€Ÿç‡**
```python
config = ScraperConfig(
    requests_per_second=2.0,  # é»˜è®¤ 1.0
)
```

2. **å‡å°‘å»¶è¿Ÿ**
```python
config = ScraperConfig(
    min_delay_seconds=0.5,   # é»˜è®¤ 1.0
    max_delay_seconds=1.5,   # é»˜è®¤ 3.0
)
```

3. **å¹¶å‘çˆ¬å–ï¼ˆé«˜çº§ï¼‰**
ç›®å‰æ˜¯ä¸²è¡Œçˆ¬å–ï¼Œå¯ä»¥æ”¹ä¸ºå¹¶å‘ï¼š
```python
# åœ¨ character_scraper.py ä¸­ä¿®æ”¹
import asyncio

async def scrape(self, character_names):
    tasks = [
        self.scrape_character(name)
        for name in character_names
    ]
    return await asyncio.gather(*tasks)
```

âš ï¸ **æ³¨æ„**ï¼šæé«˜é€Ÿç‡å¯èƒ½å¯¼è‡´è¢«ç½‘ç«™å°ç¦ï¼Œå»ºè®®ä¿æŒé»˜è®¤é…ç½®ã€‚

### Q5: å¦‚ä½•å®šæ—¶è‡ªåŠ¨çˆ¬å–ï¼Ÿ

**æ–¹æ³• 1ï¼šä½¿ç”¨ cronï¼ˆLinux/macOSï¼‰**

```bash
# ç¼–è¾‘ crontab
crontab -e

# æ·»åŠ å®šæ—¶ä»»åŠ¡ï¼ˆæ¯å¤©å‡Œæ™¨ 3 ç‚¹æ‰§è¡Œï¼‰
0 3 * * * curl -X POST http://localhost:8002/api/scraper/characters/trigger
```

**æ–¹æ³• 2ï¼šä½¿ç”¨ APSchedulerï¼ˆæ¨èï¼‰**

åœ¨ `src/main.py` æ·»åŠ ï¼š

```python
from apscheduler.schedulers.asyncio import AsyncIOScheduler

scheduler = AsyncIOScheduler()

@app.on_event("startup")
async def start_scheduler():
    # æ¯å¤© 3 ç‚¹æ‰§è¡Œ
    scheduler.add_job(
        trigger_scraping,
        'cron',
        hour=3,
        minute=0
    )
    scheduler.start()

async def trigger_scraping():
    async with AsyncSessionLocal() as db:
        await run_character_scraping(db)
```

**æ–¹æ³• 3ï¼šä½¿ç”¨ Celeryï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰**

é€‚åˆå¤§è§„æ¨¡å®šæ—¶ä»»åŠ¡ç®¡ç†ã€‚

### Q6: çˆ¬è™«è¢«å°äº†æ€ä¹ˆåŠï¼Ÿ

**è§£å†³æ–¹æ¡ˆï¼š**

1. **é™ä½è¯·æ±‚é€Ÿç‡**
```python
config = ScraperConfig(
    requests_per_second=0.5,  # æ¯ 2 ç§’ 1 ä¸ªè¯·æ±‚
    min_delay_seconds=2.0,
    max_delay_seconds=5.0,
)
```

2. **æ·»åŠ ä»£ç†**
```python
config = ScraperConfig(
    proxy_url="http://proxy.example.com:8080"
)
```

3. **è½®æ¢ User-Agent**
å·²å†…ç½®ï¼Œä¼šè‡ªåŠ¨è½®æ¢ 5 ä¸ªä¸åŒçš„ User-Agent

4. **ç­‰å¾…åé‡è¯•**
é€šå¸¸å°ç¦æ˜¯ä¸´æ—¶çš„ï¼Œç­‰å¾…å‡ å°æ—¶åå†è¯•

### Q7: å¦‚ä½•éªŒè¯æ•°æ®å®Œæ•´æ€§ï¼Ÿ

**æ–¹æ³• 1ï¼šæŸ¥çœ‹ç»Ÿè®¡**
```bash
curl http://localhost:8002/api/scraper/stats
```

**æ–¹æ³• 2ï¼šæŸ¥è¯¢æ•°æ®åº“**
```bash
# è¿æ¥æ•°æ®åº“
psql -h localhost -U genshin_user -d genshin_wiki

# ç»Ÿè®¡è§’è‰²æ•°é‡
SELECT COUNT(*) FROM characters;

# æŸ¥çœ‹æ¯ä¸ªå­—æ®µçš„è¦†ç›–ç‡
SELECT
    COUNT(*) as total,
    COUNT(name_en) as has_name_en,
    COUNT(description) as has_description,
    COUNT(base_stats) as has_base_stats,
    COUNT(ascension_stats) as has_ascension_stats
FROM characters;
```

**æ–¹æ³• 3ï¼šAPI æŸ¥è¯¢**
```bash
# æŸ¥è¯¢æ‰€æœ‰è§’è‰²
curl "http://localhost:8002/api/characters?page_size=100" | jq '.data.items[] | {name, rarity, element}'
```

---

## æ€§èƒ½æŒ‡æ ‡

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| **çˆ¬å–é€Ÿåº¦** | 1 è§’è‰²/ç§’ (é»˜è®¤é…ç½®) |
| **æˆåŠŸç‡** | 100% (18/18 æµ‹è¯•) |
| **æ•°æ®å®Œæ•´æ€§** | |
| - åŸºç¡€ä¿¡æ¯ | 100% |
| - å±æ€§æ•°æ® | 100% |
| - çªç ´å±æ€§ | 88.9% |
| - è‹±æ–‡å | 88.9% |
| - æè¿° | 72.2% |
| **å¹³å‡å“åº”æ—¶é—´** | 500-1000ms/è§’è‰² |
| **å†…å­˜å ç”¨** | < 100MB |
| **å¹¶å‘æ”¯æŒ** | æ˜¯ï¼ˆåå°ä»»åŠ¡ï¼‰ |

---

## ç»´æŠ¤å’Œç›‘æ§

### æ—¥å¿—ä½ç½®

- **åº”ç”¨æ—¥å¿—**ï¼šç»ˆç«¯è¾“å‡ºæˆ– `logs/app.log`
- **çˆ¬è™«æ—¥å¿—**ï¼šåŒ…å«åœ¨åº”ç”¨æ—¥å¿—ä¸­ï¼Œå‰ç¼€ `src.scrapers`

### ç›‘æ§æŒ‡æ ‡

1. **çˆ¬å–æˆåŠŸç‡**
```bash
curl http://localhost:8002/api/scraper/stats | jq '.data.last_result.scraper_stats.success_rate'
```

2. **æ•°æ®æ›´æ–°æƒ…å†µ**
```bash
curl http://localhost:8002/api/scraper/stats | jq '.data.last_result.storage_stats'
```

3. **é”™è¯¯æ•°é‡**
```bash
curl http://localhost:8002/api/scraper/stats | jq '.data.last_result.scraper_stats.errors'
```

---

## æŠ€æœ¯æ ˆ

| ç»„ä»¶ | æŠ€æœ¯ | ç‰ˆæœ¬ |
|------|------|------|
| **HTTP å®¢æˆ·ç«¯** | aiohttp | 3.9+ |
| **HTML è§£æ** | BeautifulSoup4 + lxml | 4.12+ |
| **å¼‚æ­¥æ¡†æ¶** | asyncio | Python 3.10+ |
| **Web æ¡†æ¶** | FastAPI | 0.104+ |
| **æ•°æ®åº“** | PostgreSQL + SQLAlchemy | 15+ / 2.0+ |
| **ç¼“å­˜** | Redis (å¯é€‰) | 7+ |

---

## ç›¸å…³èµ„æº

- **API æ–‡æ¡£**: http://localhost:8002/api/docs
- **æ•°æ®åº“è®¾è®¡**: `docs/DATABASE_SCHEMA.md`
- **ç¼“å­˜ç­–ç•¥**: `docs/CACHING_STRATEGY.md`
- **GitHub Issue #13**: è§’è‰²æ•°æ®çˆ¬è™«å®ç°

---

## æ›´æ–°æ—¥å¿—

| æ—¥æœŸ | ç‰ˆæœ¬ | æ›´æ–°å†…å®¹ |
|------|------|---------|
| 2025-11-10 | v1.0 | åˆå§‹ç‰ˆæœ¬ï¼Œæ”¯æŒè§’è‰²åŸºç¡€ä¿¡æ¯å’Œå±æ€§çˆ¬å– |
| 2025-11-10 | v1.1 | ä¿®å¤ rarity æå–é”™è¯¯ï¼Œä¼˜åŒ–å±æ€§è¡¨æ ¼è§£æ |
| 2025-11-10 | v1.2 | æ·»åŠ ç‰¹æ®Šè§’è‰²æ”¯æŒï¼ˆé‚£ç»´è±ç‰¹çš„"å¤é¾™å¤§æƒ"ï¼‰ |

---

## è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æäº¤ GitHub Issue æˆ–è”ç³»å¼€å‘å›¢é˜Ÿã€‚
