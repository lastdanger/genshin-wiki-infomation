# çˆ¬è™«ä½¿ç”¨æŒ‡å— | Scraper Guide

åŸç¥æ¸¸æˆä¿¡æ¯çˆ¬è™«ç³»ç»Ÿï¼Œç”¨äºä»å¤šä¸ªæ•°æ®æºè‡ªåŠ¨çˆ¬å–å’Œæ›´æ–°è§’è‰²ã€æ­¦å™¨ã€åœ£é—ç‰©ç­‰æ•°æ®ã€‚

## ğŸ“‘ ç›®å½•

- [åŠŸèƒ½ç‰¹æ€§](#åŠŸèƒ½ç‰¹æ€§)
- [æ•°æ®æº](#æ•°æ®æº)
- [æ¶æ„è®¾è®¡](#æ¶æ„è®¾è®¡)
- [ä½¿ç”¨æ–¹æ³•](#ä½¿ç”¨æ–¹æ³•)
- [API ç«¯ç‚¹](#api-ç«¯ç‚¹)
- [é…ç½®é€‰é¡¹](#é…ç½®é€‰é¡¹)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)

---

## åŠŸèƒ½ç‰¹æ€§

### âœ¨ æ ¸å¿ƒåŠŸèƒ½

- **å¤šæ•°æ®æºæ”¯æŒ**ï¼šæ”¯æŒä» Bilibili Game Wikiã€HomdGCat Wiki ç­‰å¤šä¸ªæ¥æºçˆ¬å–æ•°æ®
- **å¼‚æ­¥çˆ¬å–**ï¼šä½¿ç”¨ `aiohttp` å¼‚æ­¥ HTTP è¯·æ±‚ï¼Œé«˜æ•ˆå¹¶å‘çˆ¬å–
- **æ™ºèƒ½é€Ÿç‡é™åˆ¶**ï¼šè‡ªåŠ¨é™åˆ¶è¯·æ±‚é¢‘ç‡ï¼Œé¿å…å¯¹ç›®æ ‡ç½‘ç«™é€ æˆå‹åŠ›
- **è¯·æ±‚é‡è¯•æœºåˆ¶**ï¼šæŒ‡æ•°é€€é¿ç®—æ³•ï¼Œè‡ªåŠ¨é‡è¯•å¤±è´¥çš„è¯·æ±‚
- **å¢é‡æ›´æ–°**ï¼šåªæ›´æ–°æœ‰å˜åŒ–çš„æ•°æ®ï¼Œé¿å…é‡å¤å†™å…¥
- **User-Agent è½®æ¢**ï¼šéšæœºè½®æ¢ User-Agentï¼Œæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨
- **é”™è¯¯å¤„ç†**ï¼šå®Œå–„çš„é”™è¯¯æ•è·å’Œæ—¥å¿—è®°å½•

### ğŸ“Š æ•°æ®ç±»å‹

ç›®å‰æ”¯æŒçˆ¬å–ï¼š
- âœ… **è§’è‰²æ•°æ®**ï¼šåŸºç¡€ä¿¡æ¯ã€å±æ€§ã€æŠ€èƒ½ã€å¤©èµ‹ã€å‘½ä¹‹åº§ã€çªç ´ææ–™
- ğŸ”„ **æ­¦å™¨æ•°æ®**ï¼šåŸºç¡€å±æ€§ã€å‰¯è¯æ¡ã€ç‰¹æ•ˆã€é€‚é…è§’è‰²ï¼ˆTODOï¼‰
- ğŸ”„ **åœ£é—ç‰©æ•°æ®**ï¼šå¥—è£…æ•ˆæœã€è¯æ¡æ¨èï¼ˆTODOï¼‰

---

## æ•°æ®æº

### 1. Bilibili Game Wiki

- **URL**: https://wiki.biligame.com/ys/
- **ç‰¹ç‚¹**ï¼šä¸­æ–‡æ•°æ®ã€æ›´æ–°åŠæ—¶ã€å†…å®¹è¯¦ç»†
- **çˆ¬å–å†…å®¹**ï¼šè§’è‰²åˆ—è¡¨ã€è§’è‰²è¯¦æƒ…ã€æŠ€èƒ½ä¿¡æ¯

### 2. HomdGCat Wiki

- **URL**: https://homdgcat.wiki/gi/char
- **ç‰¹ç‚¹**ï¼šæ•°æ®ç»“æ„æ¸…æ™°ã€API å‹å¥½
- **çˆ¬å–å†…å®¹**ï¼šè§’è‰²ç»Ÿè®¡æ•°æ®ã€è£…å¤‡æ¨è

### âš ï¸ æ³¨æ„äº‹é¡¹

- **éµå®ˆ robots.txt**ï¼šçˆ¬è™«ä¼šæ£€æŸ¥å¹¶éµå®ˆç½‘ç«™çš„ robots.txt è§„åˆ™
- **åˆç†é€Ÿç‡**ï¼šé»˜è®¤ 1 è¯·æ±‚/ç§’ï¼Œé¿å…å¯¹ç›®æ ‡ç½‘ç«™é€ æˆå‹åŠ›
- **å‹å¥½ User-Agent**ï¼šä½¿ç”¨æ˜ç¡®æ ‡è¯†çš„ User-Agent
- **ä»…ç”¨äºå­¦ä¹ **ï¼šæœ¬çˆ¬è™«ä»…ç”¨äºä¸ªäººå­¦ä¹ å’Œéå•†ä¸šç”¨é€”

---

## æ¶æ„è®¾è®¡

### æ¨¡å—ç»“æ„

```
backend/src/scrapers/
â”œâ”€â”€ __init__.py              # æ¨¡å—å…¥å£
â”œâ”€â”€ base_scraper.py          # åŸºç¡€çˆ¬è™«ç±»
â”œâ”€â”€ character_scraper.py     # è§’è‰²æ•°æ®çˆ¬è™«
â”œâ”€â”€ data_storage.py          # æ•°æ®å­˜å‚¨æœåŠ¡
â””â”€â”€ weapon_scraper.py        # æ­¦å™¨æ•°æ®çˆ¬è™«ï¼ˆTODOï¼‰
```

### ç±»å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   BaseScraper       â”‚
â”‚  (æŠ½è±¡åŸºç±»)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ + config            â”‚
â”‚ + session           â”‚
â”‚ + fetch()           â”‚
â”‚ + parse_html()      â”‚
â”‚ + scrape() abstract â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â–²
          â”‚ ç»§æ‰¿
          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CharacterScraper    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ + scrape()          â”‚
â”‚ + scrape_list()     â”‚
â”‚ + scrape_details()  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å·¥ä½œæµç¨‹

```
1. åˆå§‹åŒ–çˆ¬è™«
   â”œâ”€â”€ åˆ›å»º HTTP Session
   â”œâ”€â”€ åŠ è½½é…ç½®
   â””â”€â”€ è®¾ç½®é€Ÿç‡é™åˆ¶

2. çˆ¬å–æ•°æ®
   â”œâ”€â”€ è·å–è§’è‰²åˆ—è¡¨
   â”‚   â”œâ”€â”€ å‘é€ HTTP è¯·æ±‚
   â”‚   â”œâ”€â”€ è§£æ HTML
   â”‚   â””â”€â”€ æå–åŸºç¡€ä¿¡æ¯
   â”‚
   â””â”€â”€ éå†è§’è‰²è¯¦æƒ…
       â”œâ”€â”€ è·å–è¯¦æƒ…é¡µ
       â”œâ”€â”€ è§£æè¯¦ç»†ä¿¡æ¯
       â””â”€â”€ åˆå¹¶æ•°æ®

3. å­˜å‚¨æ•°æ®
   â”œâ”€â”€ æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨
   â”œâ”€â”€ æ¯”è¾ƒæ˜¯å¦æœ‰å˜åŒ–
   â””â”€â”€ å¢é‡æ›´æ–°æˆ–åˆ›å»º
```

---

## ä½¿ç”¨æ–¹æ³•

### 1. é€šè¿‡ API æ‰‹åŠ¨è§¦å‘

#### è§¦å‘è§’è‰²æ•°æ®çˆ¬å–

```bash
curl -X POST http://localhost:8001/api/scraper/characters/trigger
```

#### æŸ¥çœ‹çˆ¬å–çŠ¶æ€

```bash
curl http://localhost:8001/api/scraper/status
```

#### æŸ¥çœ‹çˆ¬å–ç»Ÿè®¡

```bash
curl http://localhost:8001/api/scraper/stats
```

### 2. åœ¨ä»£ç ä¸­ä½¿ç”¨

```python
from src.scrapers.character_scraper import CharacterScraper
from src.scrapers.base_scraper import ScraperConfig
from src.scrapers.data_storage import DataStorageService
from src.db.session import get_db

# é…ç½®çˆ¬è™«
config = ScraperConfig(
    requests_per_second=1.0,  # æ¯ç§’1ä¸ªè¯·æ±‚
    max_retries=3,            # æœ€å¤šé‡è¯•3æ¬¡
    timeout_seconds=30,       # è¶…æ—¶30ç§’
)

# åˆ›å»ºçˆ¬è™«å®ä¾‹
scraper = CharacterScraper(config)

# æ‰§è¡Œçˆ¬å–
async with scraper:
    # çˆ¬å–æ‰€æœ‰è§’è‰²æ•°æ®
    characters = await scraper.scrape()

    # å­˜å‚¨åˆ°æ•°æ®åº“
    async with get_db() as db:
        storage = DataStorageService(db)
        stats = await storage.store_characters(characters)
        print(f"Created: {stats['created']}, Updated: {stats['updated']}")
```

### 3. å®šæ—¶ä»»åŠ¡ï¼ˆCeleryï¼‰

```python
# TODO: é…ç½® Celery å®šæ—¶ä»»åŠ¡
from celery import Celery
from celery.schedules import crontab

app = Celery('tasks')

@app.task
def scrape_characters():
    """æ¯å¤©å‡Œæ™¨3ç‚¹çˆ¬å–è§’è‰²æ•°æ®"""
    # çˆ¬å–é€»è¾‘
    pass

app.conf.beat_schedule = {
    'scrape-characters-daily': {
        'task': 'tasks.scrape_characters',
        'schedule': crontab(hour=3, minute=0),  # æ¯å¤©å‡Œæ™¨3ç‚¹
    },
}
```

---

## API ç«¯ç‚¹

### POST /api/scraper/characters/trigger

æ‰‹åŠ¨è§¦å‘è§’è‰²æ•°æ®çˆ¬å–ã€‚

**è¯·æ±‚ç¤ºä¾‹ï¼š**
```bash
curl -X POST http://localhost:8001/api/scraper/characters/trigger
```

**å“åº”ç¤ºä¾‹ï¼š**
```json
{
  "success": true,
  "message": "Character scraping task started in background",
  "status": "started"
}
```

### GET /api/scraper/status

è·å–çˆ¬è™«å½“å‰çŠ¶æ€ã€‚

**å“åº”ç¤ºä¾‹ï¼š**
```json
{
  "success": true,
  "data": {
    "is_running": false,
    "current_task": null,
    "last_run": "2025-11-10T10:30:00",
    "last_result": {
      "success": true,
      "scraper_stats": {
        "requests": 50,
        "errors": 2,
        "success_rate": 96.0
      },
      "storage_stats": {
        "created": 10,
        "updated": 35,
        "skipped": 3,
        "errors": 2
      },
      "total_characters": 48
    }
  }
}
```

### GET /api/scraper/stats

è·å–çˆ¬è™«ç»Ÿè®¡ä¿¡æ¯ã€‚

**å“åº”ç¤ºä¾‹ï¼š**
```json
{
  "success": true,
  "data": {
    "last_run": "2025-11-10T10:30:00",
    "is_running": false,
    "last_result": {
      "success": true,
      "total_characters": 48
    }
  }
}
```

### GET /api/scraper/config

è·å–çˆ¬è™«é…ç½®ã€‚

**å“åº”ç¤ºä¾‹ï¼š**
```json
{
  "success": true,
  "data": {
    "requests_per_second": 1.0,
    "max_retries": 3,
    "timeout_seconds": 30,
    "respect_robots_txt": true
  }
}
```

---

## é…ç½®é€‰é¡¹

### ScraperConfig å‚æ•°

```python
@dataclass
class ScraperConfig:
    # é€Ÿç‡é™åˆ¶
    requests_per_second: float = 1.0       # æ¯ç§’è¯·æ±‚æ•°
    min_delay_seconds: float = 1.0         # æœ€å°å»¶è¿Ÿï¼ˆç§’ï¼‰
    max_delay_seconds: float = 3.0         # æœ€å¤§å»¶è¿Ÿï¼ˆç§’ï¼‰

    # é‡è¯•é…ç½®
    max_retries: int = 3                   # æœ€å¤§é‡è¯•æ¬¡æ•°
    retry_delay_seconds: float = 2.0       # é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
    retry_backoff_factor: float = 2.0      # æŒ‡æ•°é€€é¿å› å­

    # è¯·æ±‚è¶…æ—¶
    timeout_seconds: int = 30              # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

    # è¿æ¥æ± 
    max_connections: int = 10              # æœ€å¤§è¿æ¥æ•°
    max_connections_per_host: int = 5      # æ¯ä¸ªä¸»æœºæœ€å¤§è¿æ¥æ•°

    # User-Agent åˆ—è¡¨
    user_agents: List[str] = [...]         # User-Agent åˆ—è¡¨

    # ä»£ç†é…ç½®ï¼ˆå¯é€‰ï¼‰
    proxy_url: Optional[str] = None        # ä»£ç† URL

    # Robots.txt
    respect_robots_txt: bool = True        # æ˜¯å¦éµå®ˆ robots.txt
```

### ç¯å¢ƒå˜é‡

å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡é…ç½®æŸäº›å‚æ•°ï¼š

```bash
# .env æ–‡ä»¶
SCRAPER_REQUESTS_PER_SECOND=0.5
SCRAPER_MAX_RETRIES=5
SCRAPER_TIMEOUT=60
SCRAPER_PROXY_URL=http://proxy.example.com:8080
```

---

## æœ€ä½³å®è·µ

### 1. é€Ÿç‡é™åˆ¶

```python
# æ¨èé…ç½®ï¼šä¿å®ˆçš„é€Ÿç‡é™åˆ¶
config = ScraperConfig(
    requests_per_second=0.5,  # 2ç§’1ä¸ªè¯·æ±‚
    min_delay_seconds=2.0,
    max_delay_seconds=5.0,
)
```

### 2. é”™è¯¯å¤„ç†

```python
try:
    async with scraper:
        characters = await scraper.scrape()
except Exception as e:
    logger.error(f"Scraping failed: {e}")
    # å‘é€å‘Šè­¦é€šçŸ¥
    send_alert(f"Scraper error: {e}")
```

### 3. å¢é‡æ›´æ–°

```python
# DataStorageService ä¼šè‡ªåŠ¨æ£€æµ‹æ•°æ®å˜åŒ–
# åªæ›´æ–°æœ‰å˜åŒ–çš„è®°å½•ï¼Œè·³è¿‡ç›¸åŒæ•°æ®
storage = DataStorageService(db)
stats = await storage.store_characters(characters)

# æŸ¥çœ‹æ›´æ–°ç»Ÿè®¡
print(f"Created: {stats['created']}")   # æ–°åˆ›å»º
print(f"Updated: {stats['updated']}")   # æ›´æ–°
print(f"Skipped: {stats['skipped']}")   # è·³è¿‡ï¼ˆæ— å˜åŒ–ï¼‰
```

### 4. ç›‘æ§å’Œæ—¥å¿—

```python
import logging

# é…ç½®è¯¦ç»†æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# çˆ¬è™«ä¼šè‡ªåŠ¨è®°å½•ï¼š
# - è¯·æ±‚ URL å’ŒçŠ¶æ€
# - é‡è¯•æ¬¡æ•°å’ŒåŸå› 
# - è§£æé”™è¯¯
# - å­˜å‚¨ç»Ÿè®¡
```

### 5. å®šæœŸæ¸…ç†

```python
# å®šæœŸæ¸…ç†è¿‡æœŸæ•°æ®
async def cleanup_old_data(db: AsyncSession):
    # åˆ é™¤3ä¸ªæœˆæœªæ›´æ–°çš„æ•°æ®
    cutoff_date = datetime.utcnow() - timedelta(days=90)
    await db.execute(
        delete(Character).where(Character.updated_at < cutoff_date)
    )
    await db.commit()
```

---

## æ•…éšœæ’æŸ¥

### é—®é¢˜ 1ï¼šçˆ¬å–å¤±è´¥

**ç—‡çŠ¶**ï¼šHTTP è¯·æ±‚å¤±è´¥ï¼Œè¿”å› 403 æˆ– 429 é”™è¯¯

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# 1. é™ä½è¯·æ±‚é¢‘ç‡
config.requests_per_second = 0.3  # æ›´ä¿å®ˆçš„é€Ÿç‡

# 2. å¢åŠ é‡è¯•æ¬¡æ•°
config.max_retries = 5

# 3. ä½¿ç”¨ä»£ç†
config.proxy_url = "http://your-proxy.com:8080"
```

### é—®é¢˜ 2ï¼šè§£æå¤±è´¥

**ç—‡çŠ¶**ï¼šæ— æ³•æå–æ•°æ®ï¼Œè¿”å› None

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# æ£€æŸ¥ç½‘ç«™ç»“æ„æ˜¯å¦å˜åŒ–
html = await scraper.fetch(url)
soup = scraper.parse_html(html)

# æ‰“å° HTML ç»“æ„
print(soup.prettify())

# è°ƒæ•´é€‰æ‹©å™¨
# ä» .character-card æ”¹ä¸º .role-box
```

### é—®é¢˜ 3ï¼šæ•°æ®åº“å†™å…¥å¤±è´¥

**ç—‡çŠ¶**ï¼šçˆ¬å–æˆåŠŸä½†å­˜å‚¨å¤±è´¥

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
for char in characters:
    if not char.get("name"):
        logger.warning(f"Missing name: {char}")
    if not char.get("element"):
        logger.warning(f"Missing element for {char.get('name')}")

# æ£€æŸ¥æ•°æ®åº“è¿æ¥
await db.execute("SELECT 1")
```

---

## å¼€å‘è®¡åˆ’

### å·²å®Œæˆ âœ…

- [x] åŸºç¡€çˆ¬è™«æ¡†æ¶
- [x] è§’è‰²æ•°æ®çˆ¬è™«
- [x] å¢é‡æ•°æ®å­˜å‚¨
- [x] API ç«¯ç‚¹
- [x] é€Ÿç‡é™åˆ¶å’Œé‡è¯•

### è¿›è¡Œä¸­ ğŸ”„

- [ ] æ­¦å™¨æ•°æ®çˆ¬è™«
- [ ] åœ£é—ç‰©æ•°æ®çˆ¬è™«
- [ ] Celery å®šæ—¶ä»»åŠ¡

### è®¡åˆ’ä¸­ ğŸ“

- [ ] æ•°æ®éªŒè¯å’Œæ¸…æ´—
- [ ] çˆ¬è™«ç›‘æ§ä»ªè¡¨ç›˜
- [ ] å¤šçº¿ç¨‹çˆ¬å–ä¼˜åŒ–
- [ ] æ•°æ®å¯¹æ¯”å’Œå˜æ›´è¿½è¸ª
- [ ] è‡ªåŠ¨åŒ–æµ‹è¯•

---

## è®¸å¯è¯

MIT License

---

**æœ€åæ›´æ–°**: 2025-11-10
